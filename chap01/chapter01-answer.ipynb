{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd652fc",
   "metadata": {},
   "source": [
    "# 線形回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be82e8",
   "metadata": {},
   "source": [
    "## 仮説・モデル表現"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c78fa0",
   "metadata": {},
   "source": [
    "- 我々は「モデル」と言う表現に親しみがあるが、界隈では昔ながらの風習で\"hypothesis\"などと呼ぶらしい。とはいえ日本語の「仮説」としての意味合いは特に無いっぽい\n",
    "- $h_\\theta$と言う関数により入力データ$x$を出力データ$y$と対応関係を作ろうとする試み\n",
    "  - $h_\\theta$は「パラメータ$\\theta$を持つ仮説」という意味\n",
    "- ここでは以下のモデルを用いて単一の出力値を予測しようとする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f3817",
   "metadata": {},
   "source": [
    "$$ \\hat{y} = h_{\\theta}(x) = \\theta_0 + \\theta_{1}x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf342b",
   "metadata": {},
   "source": [
    "予測に用いるデータは以下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10273e",
   "metadata": {},
   "source": [
    "| input | output |\n",
    "|-------|--------|\n",
    "| 0     | 3      |\n",
    "| 1     | 4      |\n",
    "| 2     | 7      |\n",
    "| 3     | 8      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b48f3",
   "metadata": {},
   "source": [
    "上記のデータセットに近い出力が得られるモデルのパラメータを$\\theta_0=2, \\theta_1=2$と仮定する。つまり $h_\\theta(x)=2+2x$とすると"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629d9fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:0 f(x):2\n",
      "x:1 f(x):4\n",
      "x:2 f(x):6\n",
      "x:3 f(x):8\n"
     ]
    }
   ],
   "source": [
    "def h(x):\n",
    "    return 2 + 2*x\n",
    "\n",
    "for x in range(4):\n",
    "    print(\"x:%s f(x):%d\" % (x, h(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d57004",
   "metadata": {},
   "source": [
    "先程のデータセットと出力結果を比較したのが以下の表、当たらずとも遠からずと言ったところ。果たしてこのモデルの精度についてどのように議論するべきなのだろうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee422c",
   "metadata": {},
   "source": [
    "| input | output | predict |\n",
    "|-------|--------|---------|\n",
    "| 0     | 3      | 2       |\n",
    "| 1     | 4      | 4       |\n",
    "| 2     | 7      | 6       |\n",
    "| 3     | 8      | 8       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42ef09",
   "metadata": {},
   "source": [
    "## 課題1. 行列式を用いてモデル表現を実装してください。\n",
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632ccdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0], [1], [2], [3]])\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "X_ = np.c_[np.ones(m), X]\n",
    "\n",
    "assert X_.shape == (4, 2) # m:4, n:2\n",
    "\n",
    "theta = np.array([[2], [2]])\n",
    "\n",
    "def h(X, theta):\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "np.testing.assert_allclose(h(X_, theta), np.array([[2], [4], [6], [8]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a12ef5",
   "metadata": {},
   "source": [
    "## コスト関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61e01b",
   "metadata": {},
   "source": [
    "モデル精度を測るために「コスト関数」と言う概念を導入する。このコスト関数は実際の出力$y$と$h_\\theta(x)$から得られた予測値$\\hat{y}$の差分の総和を平均したものとする。（実際には1/2が乗じられているが）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cb1ec",
   "metadata": {},
   "source": [
    "$$J(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}(\\hat{y} - y_i)^2 = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x_i) - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02fe1c",
   "metadata": {},
   "source": [
    "これが俗に言う「平均二乗誤差」。なぜ$\\frac{1}{2m}$かというと、あらゆる解説の中で幾度も語られていると思うが、後述する勾配降下法の計算の際に二乗関数の微分項が$\\frac{1}{2}$で上手く打ち消すことができるから。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb71a0",
   "metadata": {},
   "source": [
    "このコスト関数を最小化するモデルを追い求めていくのが回帰のタスクになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4eea0",
   "metadata": {},
   "source": [
    "平均二乗誤差を視覚化すると以下になる。ここでは青/赤のラインがoutputとpredictの乖離を表現している。我々がモデルを作る際には各出力点から予測点までの距離の和が最小化するような$\\theta$（理想的には$J(\\theta_0, \\theta_1)=0$）を選択することが求められる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7795b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x109f86df0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'input':[0, 1, 2, 3], \n",
    "     'output':[3, 5, 5, 9], \n",
    "     'predict':[2, 4, 6, 8]})\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(df['input'], df['output'], color='black', label='output', marker='^')\n",
    "plt.scatter(df['input'], df['predict'], color='black', label='predict')\n",
    "plt.plot(df['input'], df['predict'], color='black')\n",
    "for i in range(4):\n",
    "    y0 = df['predict'][i]\n",
    "    y1 = df['output'][i]\n",
    "    if (y1 - y0) > 0:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    plt.plot([i, i], [y0, y1], color=color)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3367c20",
   "metadata": {},
   "source": [
    "## 課題2. 行列式を用いてコスト関数を実装してください。\n",
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569f3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(X, y, theta):\n",
    "    \"\"\"calculate cost function\n",
    "    \n",
    "    Args:\n",
    "        - X(np.array): Input values\n",
    "        - y(np.array): Actual output values\n",
    "        - theta(np.array): parameters of hypothesis\n",
    "    \n",
    "    Returns:\n",
    "        - J(float):\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    h = np.dot(X, theta)\n",
    "    J = 1/(2*m) * np.sum((h - y)**2)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab39b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n",
    "y = np.array([7, 6, 5, 4])\n",
    "theta = np.array([0.1, 0.2])\n",
    "np.testing.assert_allclose(costFunction(X, y, theta), 11.945, rtol=1e-05)\n",
    "\n",
    "X = np.array([[1, 2, 3], [1, 3, 4], [1, 4, 5], [1, 5, 6]])\n",
    "y = np.array([[7], [6], [5], [4]])\n",
    "theta = np.array([[0.1], [0.2], [0.3]])\n",
    "np.testing.assert_allclose(costFunction(X, y, theta), 7.0175, rtol=1e-05)\n",
    "\n",
    "X = np.array([[2, 1, 3], [7, 1, 9], [1, 8, 1], [3, 7, 4]])\n",
    "y = np.array([[2], [5], [5], [6]])\n",
    "theta = np.array([[0.4], [0.6], [0.8]])\n",
    "np.testing.assert_allclose(costFunction(X, y, theta), 5.295, rtol=1e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60f21d",
   "metadata": {},
   "source": [
    "## 勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c417d83",
   "metadata": {},
   "source": [
    "すでに我々は手元のデータセットからその背後に隠れる数学的なモデルを探し出す理論を身に付けたわけだが、何か足りない気がする。その違和感とは「$J(\\theta_0, \\theta_1)=0$となるまたはそれに近い$\\theta$はそう簡単に見つかるのか？」だろう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9fae6",
   "metadata": {},
   "source": [
    "想像してみて欲しい、仮に10000点のデータセットに対してコスト関数が最小になる$\\theta$を探す作業を。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41adfcf",
   "metadata": {},
   "source": [
    "そこで登場するのが「最適化」、とはいえ大して難解なものではない。今回は最適解を解析的に解くことはせずあくまでも数値計算のみで解を近似的に求めていく。その際に有効なツールとなるのが「勾配降下法」だ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cfbc2",
   "metadata": {},
   "source": [
    "![gradient-descend-3d](img/gradient-descend-3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb9526",
   "metadata": {},
   "source": [
    "これは$x$と$y$とではなくモデルのパラメータ$\\theta$とそれを用いて計算したコスト関数のプロットである。このケースでは、仮に$\\theta_0=\\theta_1=0$からスタートしたならば直感的にどの方向にパラメーターを探索していけば良いかがわかるはずだ。これを数学的にどのように実装すれば良いのだろうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19518168",
   "metadata": {},
   "source": [
    "ここで「微分」の概念を取り入れる。具体的にはこのコスト関数の接線を降下方向に一定間隔でずらしていきその接線の傾きが0、またはそれに近しくなる点を探索していく。この時の接線をずらす間隔を「学習率」（しばしば$\\alpha$と表現する）と呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a618a72",
   "metadata": {},
   "source": [
    "上述の手順は以下の式の繰り返しで表すことができる。このとき$j$は特徴量のインデックス番号を表す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3ee76",
   "metadata": {},
   "source": [
    "$$ \\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1) \\ \\ j\\in {0, 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d77af",
   "metadata": {},
   "source": [
    "実際に線形回帰の問題に適用する際には以下のように若干の式変形を行う必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3ae76",
   "metadata": {},
   "source": [
    "$$\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x_i) - y_i){{x_i}_0}$$\n",
    "\n",
    "$$\\theta_1 := \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x_i) - y_i){{x_i}_1}$$\n",
    "\n",
    "※本講習のように定数項が常に1であるとするならば$\\theta_0$の更新式は以下のようにも表現できる\n",
    "$$\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x_i) - y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be29bacc",
   "metadata": {},
   "source": [
    "上式の導出は以下の通り。\n",
    "\n",
    "$\\frac {\\partial J(\\theta)}{\\partial \\theta_j} = \\frac {\\partial }{\\partial \\theta_j} \\frac{1}{2} (h_\\theta (x) - y)^2$  \n",
    "$\\qquad = 2 \\cdot \\frac{1}{2} (h_\\theta(x) - y) \\cdot \\frac {\\partial }{\\partial \\theta_j} (h_\\theta(x) - y)$  \n",
    "$\\qquad = (h_\\theta(x) - y) \\cdot \\frac {\\partial }{\\partial \\theta_j} \\left ( \\sum_{i=0}^n \\theta_i x_i - y_i \\right )$  \n",
    "$\\qquad = (h_\\theta(x) - y)x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4a130",
   "metadata": {},
   "source": [
    "## 実装のためのヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add9d6b",
   "metadata": {},
   "source": [
    "まず初めに先ほどの更新式における以下の項はそれぞれスカラーであることに注意する。\n",
    "$$(h_\\theta(x_i) - y_i){{x_i}_0}$$\n",
    "$$(h_\\theta(x_i) - y_i){{x_i}_1}$$\n",
    "このとき、たとえば$\\theta_0$の更新式第二項目は以下と同等であると言える。\n",
    "\n",
    "$$ (h_\\theta(x_0) - y_0){{x_0}_0} + (h_\\theta(x_1) - y_1){{x_1}_0} + \\cdots + (h_\\theta(x_m) - y_m){{x_m}_0}  $$\n",
    "\n",
    "これをさらに簡素な表現にすると以下のようなベクトルの内積として表現できる。\n",
    "\n",
    "$$ a_0b_0 + a_1b_1 + \\cdots + a_nb_n = \\vec{a} \\cdot \\vec{b} \\\\ \\vec{a}: \\vec{(h_\\theta(x_i) - y_i)}, \\ \\ \\vec{b}: \\vec{{x_i}_0}, \\ 0\\leq i \\leq m$$\n",
    "よって先ほどの更新式の第二項についてはXと(h-y)の内積を求めてやれば良いことになる。ただし二つの行列の形が内積可能であることが必要なので適宜転置などで変形しなければならない。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f282fd",
   "metadata": {},
   "source": [
    "## 勾配とは？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8d09d",
   "metadata": {},
   "source": [
    "そもそもの勾配の定義から振り返ろう。以下のような関数fについて考える。\n",
    "\n",
    "$$f(x) = 3x_1^2 - 2x_1x_2 + 3x_2^2 -4x_1 -4x_2$$\n",
    "\n",
    "![fig1](img/fig1.gif)\n",
    "\n",
    "このときベクトル$\\nabla f(x)$を点xにおける関数fの勾配(gradient)という。\n",
    "\n",
    "$$ \\nabla f(x) = \\left( \\frac{\\partial f(x)}{\\partial x_1}, \\frac{\\partial f(x)}{\\partial x_2}, \\cdots, \\frac{\\partial f(x)}{\\partial x_n} \\right) \\in \\mathbb R^n $$\n",
    "\n",
    "※ただし以下が成り立つ場合（これは2点を結ぶ線分の傾きから求められますよね？）\n",
    "\n",
    "$$ f(x+d) = f(x) + \\nabla f(x)^Td + o(\\Vert{d}\\Vert), d \\in \\mathbb R^n$$\n",
    "\n",
    "この定義に基づくと上式の勾配は以下で表すことができる。\n",
    "\n",
    "$$\\nabla f(x) = \\begin{pmatrix}\n",
    "6x_1-2x_2-4 \\\\\n",
    "-2x_1 + 6x2 -4 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "ここで勾配の定義をより直感的にするために以下の2点における勾配を考えてみる。\n",
    "\n",
    "$$a = (0, 0)^T, \\nabla f(a) = (-4, -4)^T$$\n",
    "$$b = (2, 0)^T, \\nabla f(b) = (8, -8)^T$$\n",
    "\n",
    "![fig2](img/fig2.jpg)\n",
    "\n",
    "このようにfにおける各点の勾配は接線に対して垂直なベクトルとして表される。\n",
    "そして上図の各点において傾きが最大となる方向を表し、勾配と反対になる方向が降下方向となる。\n",
    "\n",
    "つまり探索方向を勾配$\\nabla f(x)$の逆（降下）方向に定めて探索することから「勾配降下法」と言われる所以である（と思う。）\n",
    "\n",
    "![fig3](img/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe656524",
   "metadata": {},
   "source": [
    "## 表記と用語"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7864f7",
   "metadata": {},
   "source": [
    "- $A_{ij}$はi行j列の行列$A$を指す\n",
    "- n行のベクトルはn次元ベクトルを指す\n",
    "- $v_i$はベクトルのi番目の要素を指す\n",
    "- ベクトルや行列は0番目から始まる\n",
    "- 行列は大文字、ベクトルは小文字で表す\n",
    "- 「スカラー」は単一の値を示し「ベクトル」ではない\n",
    "- $\\mathbb{R}$は実数スカラーの集合\n",
    "- $\\mathbb{R}^n$は実数のn次元ベクトル集合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c100f5",
   "metadata": {},
   "source": [
    "## 行列の演算、式とコードの対応"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ba916",
   "metadata": {},
   "source": [
    "$\\sum_{i=1}^{n}x_iy_i = \\boldsymbol{x}^T\\boldsymbol{y}$ : ```np.dot(x, y)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb830d",
   "metadata": {},
   "source": [
    "$\\sum_{i=1}^{n}x_i^2$ : ```(x**2).sum()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e08d6",
   "metadata": {},
   "source": [
    "$\\frac{1}{n}(\\sum_{i=1}^{n} x_i )^2$ : ```x.sum()**2/n```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427babe5",
   "metadata": {},
   "source": [
    "## 課題3. 勾配降下法を実装してください。\n",
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, iterations):\n",
    "    \"\"\"calculate gradient descent\n",
    "    \n",
    "    Args:\n",
    "        - X(np.array): Input values\n",
    "        - y(np.array): Actual output values\n",
    "        - theta(np.array): parameters of hypothesis\n",
    "        - alpha(float): learning rate\n",
    "        - iterations(int): number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        - theta_min(np.array): parameters of hypothesis which make minimize cost function \n",
    "        - j_hist(list): history of cost \"j\"\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    j_history = []\n",
    "    for i in range(iterations):\n",
    "        h = np.dot(X, theta)\n",
    "        theta_0 = theta[0] - alpha * (1/m) * np.dot(X[:, 0].T, (h - y))\n",
    "#         theta_0 = theta[0] - alpha * (1/m) * np.sum(h - y) # equivalent to above one\n",
    "        theta_1 = theta[1] - alpha * (1/m) * np.dot(X[:, 1].T, (h - y))\n",
    "        theta = np.c_[theta_0, theta_1].reshape(-1, 1)\n",
    "        j_history.append(costFunction(X, y, theta))\n",
    "    return theta_min, j_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2828c1c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 5], [1, 2], [1, 4], [1, 5]])\n",
    "y = np.array([[1], [6], [4], [2]])\n",
    "theta = np.array([[0], [0]])\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "theta_min, j_hist = gradientDescent(X, y, theta, alpha, iterations)\n",
    "np.testing.assert_allclose(theta_min, np.array([[ 5.21475495], [-0.57334591]]), rtol=1e-07)\n",
    "\n",
    "X = np.array([[1, 5], [1, 2]])\n",
    "y = np.array([[1], [6]])\n",
    "theta = np.array([[0.5], [0.5]])\n",
    "alpha = 0.1\n",
    "iterations = 10\n",
    "\n",
    "theta_min, j_hist = gradientDescent(X, y, theta, alpha, iterations)\n",
    "np.testing.assert_allclose(theta_min, np.array([[1.70986322], [0.19229354]]), rtol=1e-07)\n",
    "theta_min, j_hist = gradientDescent(X, y, theta, alpha, iterations)\n",
    "np.testing.assert_allclose(j_hist, \n",
    "                        np.array([5.8853125, 5.7138519, 5.5475438, 5.3861213, 5.2294088, 5.0772597, 4.9295383, 4.7861152, 4.6468651, 4.5116663]),\n",
    "                        rtol=1e-07)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6639098",
   "metadata": {},
   "source": [
    "# 多変数への拡張"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c676c",
   "metadata": {},
   "source": [
    "以下の表記を導入することで複数の入力変数を取り扱えるようにする。\n",
    "\n",
    "$x_j^{(i)}$ = $j$個の特徴量を持つ$i$番目の訓練データ  \n",
    "$x^{(i)}$ = $i$番目の訓練データで全ての特徴量を含む列ベクトル  \n",
    "$m$ = 訓練データの数  \n",
    "$n$ = len($x^{(i)}$)、特徴量の数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f25bc",
   "metadata": {},
   "source": [
    "それでは多変数のモデルを以下のように定義する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d05d96",
   "metadata": {},
   "source": [
    "$$h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1+\\theta_2x_2+ \\theta_3x_3+\\cdots+\\theta_nx_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f30e89",
   "metadata": {},
   "source": [
    "上式に対する直感的な理解を育むところから始める。たとえば$\\theta_0$は家のベース価格と考えることができる。$\\theta_1$は土地の広さが価格に与える係数で$x_1$は実際の広さ、$\\theta_2$はベッドルームの数が価格に与える係数で$x_2$は実際のベッドルームの数、といった具合に考えてみてはどうだろうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c04c43",
   "metadata": {},
   "source": [
    "実装の際には行列の演算規則に従って上記のモデルを以下のように表現したものを考慮する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f949cd",
   "metadata": {},
   "source": [
    "$$h_\\theta(x) = [\\theta_0 \\quad \\theta_1 \\quad \\theta_2 \\quad \\cdots \\quad \\theta_n] \\begin{bmatrix} x_0  \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\theta^Tx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae36b9",
   "metadata": {},
   "source": [
    "これはいわゆる「ベクトル化」と呼ばれるものです。ちなみに本講習では理解を容易なものにするため$x_0^{(i)} = 1 \\ for \\ (i  \\in 1, \\ \\ldots, m)$、つまり全ての切片項は1とみなします。（実際にそれが最善の値でないことはお分かりかと思いますが）。そしてこの$x_0^{(i)} = 1$をxに追加することでthetaとxの演算ができるようになります。この時それぞれの要素数は$n+1$と同じになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f9ead4",
   "metadata": {},
   "source": [
    "たとえば訓練データは以下のようになります。以降$X$はデータ$x_{(i)}$（𝑖 番目の訓練データで全ての特徴量を含む列ベクトル）を行ごとに保持する行列を表すものとします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541e077",
   "metadata": {},
   "source": [
    "$$ X = \\begin {bmatrix} x_0^{(1)}&x_1^{(1)} \\\\ x_0^{(2)}&x_1^{(2)} \\\\ x_0^{(3)}&x_1^{(3)} \\end {bmatrix}, \\theta=\\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end {bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d8dd0",
   "metadata": {},
   "source": [
    "## コスト関数（多変数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d6c43",
   "metadata": {},
   "source": [
    "パラメーターベクトル$\\theta$のコスト関数は以下となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98a767",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}) ^ 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9ec40",
   "metadata": {},
   "source": [
    "単変数のものと異なり$x,y$の表現が多変数の導入に合わせて変わっていることを確認してほしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b40b0",
   "metadata": {},
   "source": [
    "## 多変数の勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509477c",
   "metadata": {},
   "source": [
    "こちらは単変数のそれと同じ表現が可能で、違いは**n個の特徴量に対して**計算が収束するまで以下の計算を繰り返すところになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9e59c",
   "metadata": {},
   "source": [
    "$$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)}$$\n",
    "\n",
    "$$\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_1^{(i)}$$\n",
    "\n",
    "$$\\theta_2 := \\theta_2 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_2^{(i)}$$\n",
    "\n",
    "$$\\vdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c3c9c",
   "metadata": {},
   "source": [
    "上式を一般化すると以下となる。右辺の最終項を列ベクトルに置換したものになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c8cce",
   "metadata": {},
   "source": [
    "$$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\qquad for \\quad j := 0...n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6ad3d",
   "metadata": {},
   "source": [
    "## 行列表記への変換"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823dad2",
   "metadata": {},
   "source": [
    "勾配降下法を以下のように表現する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad92a380",
   "metadata": {},
   "source": [
    "$$\\theta := \\theta - \\alpha \\nabla J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e999f",
   "metadata": {},
   "source": [
    "$\\nabla J(\\theta)$は以下の形式で表される列ベクトルとなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a5592",
   "metadata": {},
   "source": [
    "$$\\nabla J(\\theta) = \\begin {bmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_0} \\frac{\\partial J(\\theta)}{\\partial \\theta_1} \\cdots \\frac{\\partial J(\\theta)}{\\partial \\theta_n} \\end {bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d3fd0",
   "metadata": {},
   "source": [
    "この$j$番目の微分項は以下２通りの内積の和として表現可能である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7580d",
   "metadata": {},
   "source": [
    "$$\\frac {\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)} \\\\ \\qquad = \\frac{1}{m} \\sum_{i=1}^{m} x_j^{(i)}  \\cdot (h_\\theta(x^{(i)}) - y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bc037",
   "metadata": {},
   "source": [
    "ここで$x_j^{(i)}$for i=1,...,mはj番目の列のm個の要素を表しており$\\overset {\\rightarrow}{x_{j}} $と等価である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c21bca",
   "metadata": {},
   "source": [
    "さらに別の項$(h_\\theta(x^{(i)}) - y{(i)})$は予測値$h_\\theta(x^{(i)})$と真値$y^{(i)}$の偏差ベクトルとなる。以上を踏まえると上式は以下のように表現できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25280be5",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\overset {\\rightarrow T}{x_j}(X\\theta - \\overset{\\rightarrow}{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9cf629",
   "metadata": {},
   "source": [
    "最終的に勾配降下法は以下の行列式を更新していけば良いことになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e547810",
   "metadata": {},
   "source": [
    "$$\\theta := \\theta - \\frac{\\alpha}{m} X^T(X\\theta - \\overset {\\rightarrow}y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba0889",
   "metadata": {},
   "source": [
    "## 課題4. 勾配降下法を多変数に対応できるよう拡張してください。\n",
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent_multi(X, y, theta, alpha, iterations):\n",
    "    \"\"\"calculate gradient descent for multi variables\n",
    "    \n",
    "    Args:\n",
    "        - X(np.array): Input values\n",
    "        - y(np.array): Actual output values\n",
    "        - theta(np.array): parameters of hypothesis\n",
    "        - alpha(float): learning rate\n",
    "        - iterations(int): number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        - theta_min(np.array): parameters of hypothesis which make minimize cost function \n",
    "        - j_hist(list): history of cost \"j\"\n",
    "    \"\"\"\n",
    "    return theta_min, j_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ffbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[2, 1, 3], [7, 1, 9], [1, 8, 1], [3, 7, 4]])\n",
    "y = np.array([[2], [5], [5], [6]])\n",
    "theta = np.array([[0.1], [-0.2], [0.3]])\n",
    "\n",
    "theta_min, j_hist = gradientDescent_multi(X, y, theta, 0.01, 10)\n",
    "np.testing.assert_allclose(theta_min, np.array([[0.1855552 ], [0.50436048], [0.40137032]]), rtol=1e-06)\n",
    "\n",
    "theta_min, j_hist = gradientDescent_multi(X, y, theta, 0.01, 10)\n",
    "np.testing.assert_allclose(j_hist, \n",
    "                    np.array([3.6325468, 1.7660945, 1.0215168, 0.6410083, 0.4153055, 0.2722962, 0.1793844, 0.1184785, 0.0784287, 0.0520649]),\n",
    "                    rtol=1e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d67a1",
   "metadata": {},
   "source": [
    "## 特徴量の正規化(Normalization or Scaling ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650884a2",
   "metadata": {},
   "source": [
    "この効能は実際に体験してみないとわからない気がするが、一言で言うと「勾配降下法の収束のため」にこの手法を実施する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc15890",
   "metadata": {},
   "source": [
    "それぞれの入力値をほぼ同じ範囲にすることで、勾配降下法を高速化することができます。これは、θが小さい範囲では早く下降し、大きい範囲ではゆっくりと下降するため、変数が非常に不均一な場合には、最適なところまで非効率的に振動してしまうからです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df93590",
   "metadata": {},
   "source": [
    "![fig4](img/fig4.png)\n",
    "\n",
    "![fig5](img/fig5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6462d7",
   "metadata": {},
   "source": [
    "これを防ぐには、入力変数の範囲を変更して、すべての変数がほぼ同じになるようにする必要があります。理想的には以下のいずれかのようになることが望ましい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85accc5",
   "metadata": {},
   "source": [
    "$$ -1 \\leq x_{(i)} \\leq 1$$ もしくは $$ -0.5 \\leq x_{(i)} \\leq 0.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271ba4a",
   "metadata": {},
   "source": [
    "これらは厳密な要件ではなく、あくまでもスピードアップを図るためのものです。目標は、すべての入力変数を、多少の差はあれど、大体これらの範囲に収めることです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c9951",
   "metadata": {},
   "source": [
    "そのための手法として、特徴量のスケーリングと平均値の正規化があります。\n",
    "\n",
    "- 特徴量スケーリング: 入力値を入力変数の範囲（すなわち、最大値から最小値を引いたもの）で割ることで、新しい範囲が1になります。\n",
    "- 平均正規化: 入力変数の値からその入力変数の平均値を引くことで、入力変数の新しい平均値が0になります。\n",
    "\n",
    "これら2つの手法を実行するには、次の式のように入力値を調整する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5548311e",
   "metadata": {},
   "source": [
    "$$x_i := \\frac{x_i - \\mu_i}{s_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22dc2b",
   "metadata": {},
   "source": [
    "この時$\\mu_i$は全ての特徴量（i）の平均値、$s_i$は（最大値-最小値)の範囲または標準偏差です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24fe743",
   "metadata": {},
   "source": [
    "範囲で割るのと、標準偏差で割るのとでは、結果が異なることに注意してください。本講習の課題では標準偏差を使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05ffa3",
   "metadata": {},
   "source": [
    "たとえば$x_i$は家の価格範囲が1000-20000、平均値が10000だとすると正規化は以下のように施される。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b8e95",
   "metadata": {},
   "source": [
    "$$x_i := \\frac{price - 1000}{1900}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e995fd",
   "metadata": {},
   "source": [
    "## 課題5. 特徴量正規化を実装してください。\n",
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fd863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    \"\"\"Normalize feature values\n",
    "    \n",
    "    Args:\n",
    "        - X(np.array): feature values\n",
    "        \n",
    "    Returns:\n",
    "        - X_norm(np.array): feature values normalized\n",
    "    \"\"\"\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064adbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(featureNormalize(np.array([[1], [2], [3]])), np.array([[-1.], [0.], [1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff82152",
   "metadata": {},
   "source": [
    "## 本稿に入れられなかったトピック"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1eedb",
   "metadata": {},
   "source": [
    "- 多項式回帰\n",
    "- 正規方程式"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
