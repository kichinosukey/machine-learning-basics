{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd652fc",
   "metadata": {},
   "source": [
    "# 線形回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be82e8",
   "metadata": {},
   "source": [
    "## 仮説・モデル表現"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c78fa0",
   "metadata": {},
   "source": [
    "- 我々は「モデル」と言う表現に親しみがあるが、界隈では昔ながらの風習で\"hypothesis\"などと呼ぶらしい。とはいえ日本語の「仮説」としての意味合いは特に無いっぽい\n",
    "- $h_\\theta$と言う関数により入力データ$x$を出力データ$y$と対応関係を作ろうとする試み\n",
    "  - $h_\\theta$は「パラメータ$\\theta$を持つ仮説」という意味\n",
    "- ここでは以下のモデルを用いて単一の出力値を予測しようとする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f3817",
   "metadata": {},
   "source": [
    "$$ \\hat{y} = h_{\\theta}(x) = \\theta_0 + \\theta_{1}x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf342b",
   "metadata": {},
   "source": [
    "予測に用いるデータは以下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10273e",
   "metadata": {},
   "source": [
    "| input | output |\n",
    "|-------|--------|\n",
    "| 0     | 3      |\n",
    "| 1     | 4      |\n",
    "| 2     | 7      |\n",
    "| 3     | 8      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b48f3",
   "metadata": {},
   "source": [
    "上記のデータセットに近い出力が得られるモデルのパラメータを$\\theta_0=2, \\theta_1=2$と仮定する。つまり $h_\\theta(x)=2+2x$とすると"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    return 2 + 2*x\n",
    "\n",
    "for x in range(4):\n",
    "    print(\"x:%s f(x):%d\" % (x, h(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d57004",
   "metadata": {},
   "source": [
    "先程のデータセットと出力結果を比較したのが以下の表、当たらずとも遠からずと言ったところ。果たしてこのモデルの精度についてどのように議論するべきなのだろうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee422c",
   "metadata": {},
   "source": [
    "| input | output | predict |\n",
    "|-------|--------|---------|\n",
    "| 0     | 3      | 2       |\n",
    "| 1     | 4      | 4       |\n",
    "| 2     | 6      | 6       |\n",
    "| 3     | 8      | 8       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42ef09",
   "metadata": {},
   "source": [
    "## 課題1. 行列式を用いてモデル表現を実装してください。\n",
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ccdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0], [1], [2], [3]])\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "X_ = np.c_[np.ones(m), X]\n",
    "\n",
    "assert X_.shape == (4, 2) # m:4, n:2\n",
    "\n",
    "theta = np.array([[2], [2]])\n",
    "\n",
    "def h(X, theta):\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "np.testing.assert_allclose(h(X_, theta), np.array([[2], [4], [6], [8]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a12ef5",
   "metadata": {},
   "source": [
    "## コスト関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61e01b",
   "metadata": {},
   "source": [
    "モデル精度を測るために「コスト関数」と言う概念を導入する。このコスト関数は実際の出力$y$と$h_\\theta(x)$から得られた予測値$\\hat{y}$の差分の総和を平均したものとする。（実際には1/2が乗じられているが）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cb1ec",
   "metadata": {},
   "source": [
    "$$J(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}(\\hat{y} - y_i)^2 = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x_i) - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02fe1c",
   "metadata": {},
   "source": [
    "これが俗に言う「平均二乗誤差」。なぜ$\\frac{1}{2m}$かというと、あらゆる解説の中で幾度も語られていると思うが、後述する勾配降下法の計算の際に二乗関数の微分項が$\\frac{1}{2}$で上手く打ち消すことができるから。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb71a0",
   "metadata": {},
   "source": [
    "このコスト関数を最小化するモデルを追い求めていくのが回帰のタスクになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4eea0",
   "metadata": {},
   "source": [
    "平均二乗誤差を視覚化すると以下になる。ここでは青/赤のラインがoutputとpredictの乖離を表現している。我々がモデルを作る際には各出力点から予測点までの距離の和が最小化するような$\\theta$（理想的には$J(\\theta_0, \\theta_1)=0$）を選択することが求められる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7795b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'input':[0, 1, 2, 3], \n",
    "     'output':[3, 5, 5, 9], \n",
    "     'predict':[2, 4, 6, 8]})\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(df['input'], df['output'], color='black', label='output', marker='^')\n",
    "plt.scatter(df['input'], df['predict'], color='black', label='predict')\n",
    "plt.plot(df['input'], df['predict'], color='black')\n",
    "for i in range(4):\n",
    "    y0 = df['predict'][i]\n",
    "    y1 = df['output'][i]\n",
    "    if (y1 - y0) > 0:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    plt.plot([i, i], [y0, y1], color=color)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3367c20",
   "metadata": {},
   "source": [
    "## 課題2. 行列式を用いてコスト関数を実装してください。\n",
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(X, y, theta):\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab39b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n",
    "y = np.array([7, 6, 5, 4])\n",
    "theta = np.array([0.1, 0.2])\n",
    "np.testing.assert_allclose(costFunction(X, y, theta), 11.945, rtol=1e-05)\n",
    "\n",
    "X = np.array([[1, 2, 3], [1, 3, 4], [1, 4, 5], [1, 5, 6]])\n",
    "y = np.array([[7], [6], [5], [4]])\n",
    "theta = np.array([[0.1], [0.2], [0.3]])\n",
    "np.testing.assert_allclose(costFunction(X, y, theta), 7.0175, rtol=1e-05)\n",
    "\n",
    "X = np.array([[2, 1, 3], [7, 1, 9], [1, 8, 1], [3, 7, 4]])\n",
    "y = np.array([[2], [5], [5], [6]])\n",
    "theta = np.array([[0.4], [0.6], [0.8]])\n",
    "np.testing.assert_allclose(costFunction(X, y, theta), 5.295, rtol=1e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60f21d",
   "metadata": {},
   "source": [
    "## 勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c417d83",
   "metadata": {},
   "source": [
    "すでに我々は手元のデータセットからその背後に隠れる数学的なモデルを探し出す理論を身に付けたわけだが、何か足りない気がする。その違和感とは「$J(\\theta_0, \\theta_1)=0$となるまたはそれに近い$\\theta$はそう簡単に見つかるのか？」だろう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9fae6",
   "metadata": {},
   "source": [
    "想像してみて欲しい、仮に10000点のデータセットに対してコスト関数が最小になる$\\theta$を探す作業を。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41adfcf",
   "metadata": {},
   "source": [
    "そこで登場するのが「最適化」、とはいえ大して難解なものではない。今回は最適解を解析的に解くことはせずあくまでも数値計算のみで解を近似的に求めていく。その際に有効なツールとなるのが「勾配降下法」だ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "filename = 'gradient-descend-3d.png'\n",
    "\n",
    "Image(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb9526",
   "metadata": {},
   "source": [
    "これは$x$と$y$とではなくモデルのパラメータ$\\theta$とそれを用いて計算したコスト関数のプロットである。このケースでは、仮に$\\theta_0=\\theta_1=0$からスタートしたならば直感的にどの方向にパラメーターを探索していけば良いかがわかるはずだ。これを数学的にどのように実装すれば良いのだろうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19518168",
   "metadata": {},
   "source": [
    "ここで「微分」の概念を取り入れる。具体的にはこのコスト関数の接線を降下方向に一定間隔でずらしていきその接線の傾きが0、またはそれに近しくなる点を探索していく。この時の接線をずらす間隔を「学習率」（しばしば$\\alpha$と表現する）と呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a618a72",
   "metadata": {},
   "source": [
    "上述の手順は以下の式の繰り返しで表すことができる。このとき$j$は特徴量のインデックス番号を表す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3ee76",
   "metadata": {},
   "source": [
    "$$ \\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1) \\ \\ j\\in {0, 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d77af",
   "metadata": {},
   "source": [
    "実際に線形回帰の問題に適用する際には以下のように若干の式変形を行う必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3ae76",
   "metadata": {},
   "source": [
    "$$\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x_i) - y_i)$$\n",
    "$$\\theta_1 := \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x_i) - y_i)x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be29bacc",
   "metadata": {},
   "source": [
    "上式の導出は以下の通り。\n",
    "\n",
    "$\\frac {\\partial J(\\theta)}{\\partial \\theta_j} = \\frac {\\partial }{\\partial \\theta_j} \\frac{1}{2} (h_\\theta (x) - y)^2$  \n",
    "$\\qquad = 2 \\cdot \\frac{1}{2} (h_\\theta(x) - y) \\cdot \\frac {\\partial }{\\partial \\theta_j} (h_\\theta(x) - y)$  \n",
    "$\\qquad = (h_\\theta(x) - y) \\cdot \\frac {\\partial }{\\partial \\theta_j} \\left ( \\sum_{i=0}^n \\theta_i x_i - y_i \\right )$  \n",
    "$\\qquad = (h_\\theta(x) - y)x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908541c6",
   "metadata": {},
   "source": [
    "## 降下方向について（時間があれば追記）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe656524",
   "metadata": {},
   "source": [
    "## 表記と用語"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7864f7",
   "metadata": {},
   "source": [
    "- $A_{ij}$はi行j列の行列$A$を指す\n",
    "- n行のベクトルはn次元ベクトルを指す\n",
    "- $v_i$はベクトルのi番目の要素を指す\n",
    "- ベクトルや行列は0番目から始まる\n",
    "- 行列は大文字、ベクトルは小文字で表す\n",
    "- 「スカラー」は単一の値を示し「ベクトル」ではない\n",
    "- $\\mathbb{R}$は実数スカラーの集合\n",
    "- $\\mathbb{R}^n$は実数のn次元ベクトル集合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c100f5",
   "metadata": {},
   "source": [
    "## 行列の演算、式とコードの対応"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ba916",
   "metadata": {},
   "source": [
    "$\\sum_{i=1}^{n}x_iy_i = \\boldsymbol{x}^T\\boldsymbol{y}$ : ```np.dot(x, y)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb830d",
   "metadata": {},
   "source": [
    "$\\sum_{i=1}^{n}x_i^2$ : ```(x**2).sum()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e08d6",
   "metadata": {},
   "source": [
    "$\\frac{1}{n}(\\sum_{i=1}^{n})^2$ : ```x.sum()**2/n```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427babe5",
   "metadata": {},
   "source": [
    "## 課題3. 勾配降下法を実装してください。\n",
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, iterations):\n",
    "    return theta_min, j_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828c1c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 5], [1, 2], [1, 4], [1, 5]])\n",
    "y = np.array([[1], [6], [4], [2]])\n",
    "theta = np.array([[0], [0]])\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "theta_min, j_hist = gradientDescent(X, y, theta, alpha, iterations)\n",
    "np.testing.assert_allclose(theta_min, np.array([[ 5.21475495], [-0.57334591]]), rtol=1e-07)\n",
    "\n",
    "X = np.array([[1, 5], [1, 2]])\n",
    "y = np.array([[1], [6]])\n",
    "theta = np.array([[0.5], [0.5]])\n",
    "alpha = 0.1\n",
    "iterations = 10\n",
    "\n",
    "theta_min, j_hist = gradientDescent(X, y, theta, alpha, iterations)\n",
    "np.testing.assert_allclose(theta_min, np.array([[1.70986322], [0.19229354]]), rtol=1e-07)\n",
    "theta_min, j_hist = gradientDescent(X, y, theta, alpha, iterations)\n",
    "np.testing.assert_allclose(j_hist, \n",
    "                        np.array([5.8853125, 5.7138519, 5.5475438, 5.3861213, 5.2294088, 5.0772597, 4.9295383, 4.7861152, 4.6468651, 4.5116663]),\n",
    "                        rtol=1e-07)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6639098",
   "metadata": {},
   "source": [
    "# 多変数への拡張"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c676c",
   "metadata": {},
   "source": [
    "以下の表記を導入することで複数の入力変数を取り扱えるようにする。\n",
    "\n",
    "$x_j^{(i)}$ = $j$個の特徴量を持つ$i$番目の訓練データ  \n",
    "$x^{(i)}$ = $i$番目の訓練データで全ての特徴量を含む列ベクトル  \n",
    "$m$ = 訓練データの数  \n",
    "$n$ = len($x^{(i)}$)、特徴量の数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f25bc",
   "metadata": {},
   "source": [
    "それでは多変数のモデルを以下のように定義する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d05d96",
   "metadata": {},
   "source": [
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x_1+\\theta_2x_2+ \\theta_3x_3+\\cdots+\\theta_nx_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f30e89",
   "metadata": {},
   "source": [
    "上式に対する直感的な理解を育むところから始める。たとえば$\\theta_0$は家のベース価格と考えることができる。$\\theta_1$は土地の広さが価格に与える係数で$x_1$は実際の広さ、$\\theta_2$はベッドルームの数が価格に与える係数で$x_2$は実際のベッドルームの数、といった具合に考えてみてはどうだろうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c04c43",
   "metadata": {},
   "source": [
    "実装の際には行列の演算規則に従って上記のモデルを以下のように表現したものを考慮する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f949cd",
   "metadata": {},
   "source": [
    "$$h_\\theta(x) = [\\theta_0 \\quad \\theta_1 \\quad \\theta_2 \\quad \\cdots \\quad \\theta_n] \\begin{bmatrix} x_0  \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = \\theta^Tx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae36b9",
   "metadata": {},
   "source": [
    "これはいわゆる「ベクトル化」と呼ばれるものです。ちなみに本講習では理解を容易なものにするため$x_0^{(i)} = 1 \\ for \\ (i  \\in 1, \\ \\ldots, m)$、つまり全ての切片項は1とみなします。（実際にそれが最善の値でないことはお分かりかと思いますが）。そしてこの$x_0^{(i)} = 1$をxに追加することでthetaとxの演算ができるようになります。この時それぞれの要素数は$n+1$と同じになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f9ead4",
   "metadata": {},
   "source": [
    "たとえば訓練データは以下のようになります。以降$X$はデータ$x_{(i)}$（𝑖 番目の訓練データで全ての特徴量を含む列ベクトル）を行ごとに保持する行列を表すものとします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541e077",
   "metadata": {},
   "source": [
    "$$ X = \\begin {bmatrix} x_0^{(1)}&x_1^{(1)} \\\\ x_0^{(2)}&x_1^{(2)} \\\\ x_0^{(3)}&x_1^{(3)} \\end {bmatrix}, \\theta=\\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end {bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d8dd0",
   "metadata": {},
   "source": [
    "## コスト関数（多変数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d6c43",
   "metadata": {},
   "source": [
    "パラメーターベクトル$\\theta$のコスト関数は以下となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98a767",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}) ^ 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9ec40",
   "metadata": {},
   "source": [
    "単変数のものと異なり$x,y$の表現が多変数の導入に合わせて変わっていることを確認してほしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b40b0",
   "metadata": {},
   "source": [
    "## 多変数の勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509477c",
   "metadata": {},
   "source": [
    "こちらは単変数のそれと同じ表現が可能で、違いは**n個の特徴量に対して**計算が収束するまで以下の計算を繰り返すところになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9e59c",
   "metadata": {},
   "source": [
    "$$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)}$$\n",
    "\n",
    "$$\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_1^{(i)}$$\n",
    "\n",
    "$$\\theta_2 := \\theta_2 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_2^{(i)}$$\n",
    "\n",
    "$$\\vdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c3c9c",
   "metadata": {},
   "source": [
    "上式を一般化すると以下となる。右辺の最終項を列ベクトルに置換したものになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c8cce",
   "metadata": {},
   "source": [
    "$$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\qquad for \\quad j := 0...n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6ad3d",
   "metadata": {},
   "source": [
    "## 行列表記への変換"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823dad2",
   "metadata": {},
   "source": [
    "勾配降下法を以下のように表現する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad92a380",
   "metadata": {},
   "source": [
    "$$\\theta := \\theta - \\alpha \\nabla J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e999f",
   "metadata": {},
   "source": [
    "$\\nabla J(\\theta)$は以下の形式で表される列ベクトルとなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a5592",
   "metadata": {},
   "source": [
    "$$\\nabla J(\\theta) = \\begin {bmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_0} \\frac{\\partial J(\\theta)}{\\partial \\theta_1} \\cdots \\frac{\\partial J(\\theta)}{\\partial \\theta_n} \\end {bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d3fd0",
   "metadata": {},
   "source": [
    "この$j$番目の微分項は以下２通りの内積の和として表現可能である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7580d",
   "metadata": {},
   "source": [
    "$$\\frac {\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)} \\\\ \\qquad = \\frac{1}{m} \\sum_{i=1}^{m} x_j^{(i)}  \\cdot (h_\\theta(x^{(i)}) - y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bc037",
   "metadata": {},
   "source": [
    "ここで$x_j^{(i)}$for i=1,...,mはj番目の列のm個の要素を表しており$\\overset {\\rightarrow}{x_{j}} $と等価である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c21bca",
   "metadata": {},
   "source": [
    "さらに別の項$(h_\\theta(x^{(i)}) - y{(i)})$は予測値$h_\\theta(x^{(i)})$と真値$y^{(i)}$の偏差ベクトルとなる。以上を踏まえると上式は以下のように表現できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25280be5",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\overset {\\rightarrow T}{x_j}(X\\theta - \\overset{\\rightarrow}{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9cf629",
   "metadata": {},
   "source": [
    "最終的に勾配降下法は以下の行列式を更新していけば良いことになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e547810",
   "metadata": {},
   "source": [
    "$$\\theta := \\theta - \\frac{\\alpha}{m} X^T(X\\theta - \\overset {\\rightarrow}y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba0889",
   "metadata": {},
   "source": [
    "## 課題4. 勾配降下法を多変数に対応できるよう拡張してください。\n",
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, iterations):\n",
    "    return theta_min, j_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ffbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[2, 1, 3], [7, 1, 9], [1, 8, 1], [3, 7, 4]])\n",
    "y = np.array([[2], [5], [5], [6]])\n",
    "theta = np.array([[0.1], [-0.2], [0.3]])\n",
    "\n",
    "theta_min, j_hist = gradientDescent(X, y, theta, 0.01, 10)\n",
    "np.testing.assert_allclose(theta_min, np.array([[0.1855552 ], [0.50436048], [0.40137032]]), rtol=1e-06)\n",
    "\n",
    "theta_min, j_hist = gradientDescent(X, y, theta, 0.01, 10)\n",
    "np.testing.assert_allclose(j_hist, \n",
    "                    np.array([3.6325468, 1.7660945, 1.0215168, 0.6410083, 0.4153055, 0.2722962, 0.1793844, 0.1184785, 0.0784287, 0.0520649]),\n",
    "                    rtol=1e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d67a1",
   "metadata": {},
   "source": [
    "## 特徴量の正規化(Normalization or Scaling ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650884a2",
   "metadata": {},
   "source": [
    "この効能は実際に体験してみないとわからない気がするが、一言で言うと「勾配降下法の収束のため」にこの手法を実施する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc15890",
   "metadata": {},
   "source": [
    "それぞれの入力値をほぼ同じ範囲にすることで、勾配降下法を高速化することができます。これは、θが小さい範囲では早く下降し、大きい範囲ではゆっくりと下降するため、変数が非常に不均一な場合には、最適なところまで非効率的に振動してしまうからです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6462d7",
   "metadata": {},
   "source": [
    "これを防ぐには、入力変数の範囲を変更して、すべての変数がほぼ同じになるようにする必要があります。理想的には以下のいずれかのようになることが望ましい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85accc5",
   "metadata": {},
   "source": [
    "$$ -1 \\leq x_{(i)} \\leq 1$$ もしくは $$ -0.5 \\leq x_{(i)} \\leq 0.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271ba4a",
   "metadata": {},
   "source": [
    "これらは厳密な要件ではなく、あくまでもスピードアップを図るためのものです。目標は、すべての入力変数を、多少の差はあれど、大体これらの範囲に収めることです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c9951",
   "metadata": {},
   "source": [
    "そのための手法として、特徴量のスケーリングと平均値の正規化があります。特徴量スケーリングでは、入力値を入力変数の範囲（すなわち、最大値から最小値を引いたもの）で割ることで、新しい範囲が1になります。平均正規化では、入力変数の値からその入力変数の平均値を引くことで、入力変数の新しい平均値が0になります。この2つの手法を実行するには、次の式のように入力値を調整します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5548311e",
   "metadata": {},
   "source": [
    "$$x_i := \\frac{x_i - \\mu_i}{s_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22dc2b",
   "metadata": {},
   "source": [
    "この時$\\mu_i$は全ての特徴量（i）の平均値、$s_i$は（最大値-最小値)の範囲または標準偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24fe743",
   "metadata": {},
   "source": [
    "範囲で割るのと、標準偏差で割るのとでは、結果が異なることに注意してください。このコースの小テストでは範囲を使い、プログラミング演習では標準偏差を使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05ffa3",
   "metadata": {},
   "source": [
    "たとえば$x_i$は家の価格範囲が1000-20000、平均値が10000だとすると正規化は以下のように施される。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b8e95",
   "metadata": {},
   "source": [
    "$$x_i := \\frac{price - 1000}{1900}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e995fd",
   "metadata": {},
   "source": [
    "## 課題5. 特徴量正規化を実装してください。\n",
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fd863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064adbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_equal(featureNormalize(np.array([[1], [2], [3]])), np.array([[-1.], [0.], [1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff82152",
   "metadata": {},
   "source": [
    "## 本稿に入れられなかったトピック"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1eedb",
   "metadata": {},
   "source": [
    "- 多項式回帰\n",
    "- 正規方程式"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
