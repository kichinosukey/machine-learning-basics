{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589af799",
   "metadata": {},
   "source": [
    "# ロジスティック回帰\n",
    "- 「回帰」という名前ではあるが分類問題を解くものである\n",
    "- このアルゴリズムを実装していく\n",
    "\n",
    "# 2値分類\n",
    "\n",
    "ここからは出力ベクトル$y$は連続値から0もしくは1の値となる。\n",
    "$$ y \\in \\{0, 1\\}$$\n",
    "このとき0は陰性、1は陽性などとして表現されるがいずれも定まった表現は無いとのこと。\n",
    "\n",
    "このように二つの分類を考えることを「二値分類問題」と呼ぶ。\n",
    "\n",
    "今までのように線形回帰を使ってこの問題を解く場合にはあらゆる予測結果をマッピングしておいて、それが0.5を越えれば1、越えなければ0と分類するやり方が一つ考えられるだろう。\n",
    "\n",
    "しかしながらこの方法ではこの問題を上手く解くことができない。その原因は多くの問題が非線形であることに起因する。\n",
    "\n",
    "## 仮説表現\n",
    "\n",
    "我々の仮説は以下を満たすものとする。要はこの仮説が出力するのは確率であると言える。\n",
    "\n",
    "$$0 \\leq h_\\theta(x) \\leq 1$$\n",
    "\n",
    "このとき「シグモイド関数」という新たな形式を取り入れると、仮説表現は以下のようになる。\n",
    "\n",
    "- $ h_\\theta(x) = g(\\theta^Tx)$ : 任意の実数$\\theta^Tx$の$g$という関数による写像が$h_\\theta(x)$   \n",
    "- $ z = \\theta^Tx $ : 特徴量$\\theta$とデータ$x$の積を$z$により抽象化  \n",
    "- $ g(z) = \\frac{1}{1 + e^{-z}} $ : $g$による$z$の写像がシグモイド関数またはロジスティック関数  \n",
    "\n",
    "![https://en.wikipedia.org/wiki/Logistic_function](logistic-function.png)  \n",
    "\n",
    "この$g(z)$はどんな実数も$\\{0, 1\\}$区間にマップするため任意の値を持つ関数を分類に適したより良い関数に変換することができる。1に近ければ1だし、0に近ければ0という容易な判断ができるようになった。\n",
    "\n",
    "たとえば$g(z)$は結果が1と判断できるように確率$h_\\theta(x) = 0.7 $を我々に与えてくれる。\n",
    "\n",
    "結果が1もしくは0と判断できる確率は以下のように表現される。\n",
    "\n",
    "$$ h_\\theta(x) = P(y = 1 | x; \\theta) = 1 - P(y = 0 | x; \\theta) $$\n",
    "$$ P(y = 0 | x; \\theta) + P(y = 1 | x; \\theta) = 1 $$\n",
    "\n",
    "これはつまり予測結果が1の確率が70%ならば0である確率は30%であるということだ。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97eef23",
   "metadata": {},
   "source": [
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e321b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(X):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8730917",
   "metadata": {},
   "source": [
    "### test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0606ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(sigmoid(-5), 0.0066929, rtol=1e-05)\n",
    "np.testing.assert_allclose(sigmoid(0), 0.50000, rtol=1e-05)\n",
    "np.testing.assert_allclose(sigmoid(5), 0.99331, rtol=1e-05)\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    sigmoid(np.array([4, 5, 6])), np.array([0.98201, 0.99331, 0.99753]), rtol=1e-05)\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    sigmoid(np.array([[-1], [0], [1]])), np.array([[0.26894], [0.50000], [0.73106]]), rtol=1e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473bc618",
   "metadata": {},
   "source": [
    "## 決定境界\n",
    "\n",
    "\n",
    "0か1かの分類結果を得るために我々は以下の仮説表現に従って得られた確率から0/1を判断することができる。\n",
    "\n",
    "$$ h_\\theta(x) \\geq 0.5 \\rightarrow y = 1 $$\n",
    "\n",
    "$$ h_\\theta(x) < 0.5 \\rightarrow y = 0 $$\n",
    "\n",
    "$ g(z) = \\frac{1}{1 + e^{-z}} $はその入力が0以上であるときその出力が0.5以上となるなど、その振る舞いは以下のように定義される。\n",
    "\n",
    "- $ z = 0, e^0 = 1 \\Rightarrow g(z) = 1/2 $\n",
    "- $ z \\rightarrow \\infty, e^{-\\infty} \\rightarrow 0 \\Rightarrow g(z) = 1 $\n",
    "- $ z \\rightarrow -\\infty, e^{\\infty} \\rightarrow \\infty \\Rightarrow g(z) = 0 $\n",
    "\n",
    "つまりgに対する入力が$\\theta^TX$ならば\n",
    "\n",
    "$$h_\\theta(x) = g(\\theta^Tx) \\geq 0.5 \\ \\ when \\ \\ \\theta^Tx \\geq 0 $$\n",
    "\n",
    "よって以下が言える\n",
    "\n",
    "$$\\theta^Tx \\geq 0 \\Rightarrow y = 1 $$\n",
    "$$\\theta^Tx \\leq 0 \\Rightarrow y = 0 $$\n",
    "\n",
    "そして決定境界は$y=0$と$y=1$となるエリアを分割する線のことであり、以下のような仮説関数($e.g. z = \\theta_0 + \\theta_1x_1+\\theta_2x_2$)によって定義される。\n",
    "\n",
    "$$ \\theta = \\begin{bmatrix} 5 \\\\ -1 \\\\ 0 \\end{bmatrix}$$\n",
    "$$ y = 1 \\ \\ if \\ \\  5 + (-1)x_1 + 0x_2 \\geq 0 $$\n",
    "$$ 5 - x_1 \\geq 0 $$\n",
    "$$ x_1 \\leq 5 $$\n",
    "\n",
    "この場合、決定境界は垂直な直線で$x_1 = 5$で表現され、その左は全て$y = 1$右は$y=0$となる。\n",
    "\n",
    "そしてシグモイド関数$g(z)$への入力は線形である必要がなく円形（$e.g. z = \\theta_0 + \\theta_1x_1^2+\\theta_2x_2^2$）のような形でも構わない。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71b11d",
   "metadata": {},
   "source": [
    "## コスト関数\n",
    "\n",
    "ここでも線形回帰とは異なる関数を使う必要がある。何故ならばロジスティック関数の出力は波状であり多くの局所最適解が存在するため、非線形関数の二乗和は非凸（局所最適解がたくさんある）になる可能性があるからである。つまり線形回帰におけるコスト関数をロジスティック関数に適用すると凸関数であることが保証できないということだ。\n",
    "\n",
    "代わりに以下のコスト関数を使う。\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}Cost(h_\\theta(x^{(i)}), y^{(i)})$$　　\n",
    "$$ Cost(h_\\theta(x), y) = -log(h_\\theta(x)) \\ \\ if \\ \\ y=1 $$　　\n",
    "$$ Cost(h_\\theta(x), y) = -log((1 - h_\\theta(x)) \\ \\ if \\ \\ y=0 $$　　\n",
    "\n",
    "![y=1](lr-cost-function-y1.png)\n",
    "![y=0](lr-cost-function-y0.png)\n",
    "\n",
    "このコスト関数の見方は仮説の予測結果がyから離れれば離れるほどコスト関数の出力は大きくなる。逆に予測結果がyに等しいとコスト関数の出力は0に近づく。\n",
    "\n",
    "$$ Cost(h_\\theta(x), y) = 0 \\ \\ if \\ \\ h_\\theta(x) = y $$\n",
    "\n",
    "また$y$が0の場合、仮説関数も0を出力すればコスト関数は0になります。仮説が1に近づけば、コスト関数は無限大に近づくことになります。\n",
    "\n",
    "$$ Cost(h_\\theta(x), y) \\rightarrow \\infty \\ \\ if \\ \\ y=0 \\ and \\ h_\\theta(x) \\rightarrow 1 $$\n",
    "\n",
    "$y$ が 1の場合、仮説関数が1を出力すればコスト関数は 0 になります。我々の仮説が 0 に近づけば、コスト関数は無限大に近づきます。\n",
    "\n",
    "$$ Cost(h_\\theta(x), y) \\rightarrow \\infty \\ \\ if \\ \\ y=1 \\ and \\ h_\\theta(x) \\rightarrow 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda322be",
   "metadata": {},
   "source": [
    "## 単純化したコスト関数と勾配効果法\n",
    "\n",
    "これまでコスト関数を二つに場合分けしていたが、以下のように一つにまとめることができる。\n",
    "\n",
    "$$ Cost(h_\\theta(x), y) = -ylog(h_\\theta(x)) - (1 - y)log(1 - h_\\theta(x)) $$\n",
    "\n",
    "上式から$y$が1の時は第二項$(1 - y)log(1 - h_\\theta(x))$は0となり、もし$y$が0の時は第一項$-ylog(h_\\theta(x))$が0になることに注目されたい。\n",
    "\n",
    "以上よりコスト関数全体を以下のように表現することができる。\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}[y^{(i)}log(h_\\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$$\n",
    "\n",
    "そもそもなぜこういったコスト関数に少し違う角度から紐解いていく。なぜ両辺の対数を取るのかは置いておいて、まず冒頭の振り返りから。\n",
    "\n",
    "$$ h_\\theta(x) = P(y = 1 | x; \\theta) = 1 - P(y = 0 | x; \\theta) $$  \n",
    "$$ P(y = 0 | x; \\theta) + P(y = 1 | x; \\theta) = 1 $$  \n",
    "\n",
    "これは以下のように書き換えることができる。\n",
    "\n",
    "$P(Y = y | X = x)  = P(Y = 1 | X = x)^y P(Y = 0 | X = x)^{1-y}$  \n",
    "$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ = h_{\\theta}(x)^y (1 - h_{\\theta}(x))^{1-y}$\n",
    "\n",
    "次に特徴量行列$X$とラベルベクトルyが与えられた時には、その$X$から$y$が生じる確率を考える。行列$X$の$i$行目のサンプルがラベル$y_i$に分類される確率を全て掛け算したものとして考えることができ、次のように表せられる。\n",
    "\n",
    "$P(y|X) = \\prod_{i=1}^{m} \\left [ h_{\\theta}(x^{(i)})^{y^{(i)}} (1 - h_{\\theta}(x^{(i)}))^{1-y^{(i)}} \\right ]$\n",
    "\n",
    "この確率を最大化することを考えるが、掛け算の形式だと考えづらい（らしい）ので対数をとってマイナスをつけたものをコスト関数として扱う。\n",
    "\n",
    "$-logP(y|X) = -\\frac{1}{m} \\sum_{i=1}^{m}[y^{(i)}log(h_\\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$\n",
    "\n",
    "ベクトル化による実装は以下のようになる。\n",
    "\n",
    "$$ h = g(X\\theta) $$\n",
    "$$ J(\\theta) = \\frac{1}{m} \\cdot (-y^T log(h) - (1 - y)^T log(1 - h)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5f11d",
   "metadata": {},
   "source": [
    "### 勾配降下法\n",
    "\n",
    "勾配降下法の一般的な式は以下のようなものであったことを思い出してほしい。\n",
    "\n",
    "$$ Repeat\\ \\  {\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)} $$\n",
    "\n",
    "微分の部分を計算すると、次のようになる。\n",
    "\n",
    "$$\\begin{align*}\n",
    "& Repeat \\; \\lbrace \\newline\n",
    "& \\; \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\newline & \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "これは線形回帰で使用していたものと同じであり、$\\theta$の全ての値を同時に更新する必要がある。\n",
    "\n",
    "ベクトル化による実装は以下のようになる。\n",
    "\n",
    "$$ \\theta := \\theta - \\frac{\\alpha}{m}X^T(g(X\\theta) - \\overset{\\rightarrow}{y}) $$\n",
    "\n",
    "まずシグモイド関数の微分を計算します（これは$J(\\theta)$の偏微分を求めるときに役立ちます）。\n",
    "\n",
    "$\\begin{align*}\\sigma(x)'&=\\left(\\frac{1}{1+e^{-x}}\\right)'=\\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\\frac{e^{-x}}{(1+e^{-x})^2} \\newline &=\\left(\\frac{1}{1+e^{-x}}\\right)\\left(\\frac{e^{-x}}{1+e^{-x}}\\right)=\\sigma(x)\\left(\\frac{+1-1 + e^{-x}}{1+e^{-x}}\\right)=\\sigma(x)\\left(\\frac{1 + e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}\\right)=\\sigma(x)(1 - \\sigma(x))\\end{align*}$\n",
    "\n",
    "これで、結果として得られる偏微分を求める準備ができました。\n",
    "\n",
    "$\\begin{align*}\\frac{\\partial}{\\partial \\theta_j} J(\\theta) &= \\frac{\\partial}{\\partial \\theta_j} \\frac{-1}{m}\\sum_{i=1}^m \\left [ y^{(i)} log (h_\\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\\theta(x^{(i)})) \\right ] \\newline&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} \\frac{\\partial}{\\partial \\theta_j} log (h_\\theta(x^{(i)}))   + (1-y^{(i)}) \\frac{\\partial}{\\partial \\theta_j} log (1 - h_\\theta(x^{(i)}))\\right ] \\newline&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j} h_\\theta(x^{(i)})}{h_\\theta(x^{(i)})}   + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 - h_\\theta(x^{(i)}))}{1 - h_\\theta(x^{(i)})}\\right ] \\newline&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j} \\sigma(\\theta^T x^{(i)})}{h_\\theta(x^{(i)})}   + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 - \\sigma(\\theta^T x^{(i)}))}{1 - h_\\theta(x^{(i)})}\\right ] \\newline&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     \\frac{y^{(i)} \\sigma(\\theta^T x^{(i)}) (1 - \\sigma(\\theta^T x^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{h_\\theta(x^{(i)})}   + \\frac{- (1-y^{(i)}) \\sigma(\\theta^T x^{(i)}) (1 - \\sigma(\\theta^T x^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 - h_\\theta(x^{(i)})}\\right ] \\newline&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     \\frac{y^{(i)} h_\\theta(x^{(i)}) (1 - h_\\theta(x^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{h_\\theta(x^{(i)})}   - \\frac{(1-y^{(i)}) h_\\theta(x^{(i)}) (1 - h_\\theta(x^{(i)})) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 - h_\\theta(x^{(i)})}\\right ] \\newline&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} (1 - h_\\theta(x^{(i)})) x^{(i)}_j - (1-y^{(i)}) h_\\theta(x^{(i)}) x^{(i)}_j\\right ] \\newline&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} (1 - h_\\theta(x^{(i)})) - (1-y^{(i)}) h_\\theta(x^{(i)}) \\right ] x^{(i)}_j \\newline&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} - y^{(i)} h_\\theta(x^{(i)}) - h_\\theta(x^{(i)}) + y^{(i)} h_\\theta(x^{(i)}) \\right ] x^{(i)}_j \\newline&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - h_\\theta(x^{(i)}) \\right ] x^{(i)}_j  \\newline&= \\frac{1}{m}\\sum_{i=1}^m \\left [ h_\\theta(x^{(i)}) - y^{(i)} \\right ] x^{(i)}_j\\end{align*}$\n",
    "\n",
    "このベクトル化実装は以下のようになる。\n",
    "\n",
    "$$  \\nabla J(\\theta) = \\frac{1}{m} \\cdot X^T \\cdot (g(X \\cdot \\theta) - \\overset{\\rightarrow}{y})  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43afdeb2",
   "metadata": {},
   "source": [
    "### ※YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrCostFunction(X, y, theta):\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0432cdec",
   "metadata": {},
   "source": [
    "### test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f626d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([[-2],[-1], [1], [2]])\n",
    "X = np.array([\n",
    "    [0.1, 0.6, 1.1],\n",
    "    [0.2, 0.7, 1.2],\n",
    "    [0.3, 0.8, 1.3],\n",
    "    [0.4, 0.9, 1.4],\n",
    "    [0.5, 1.0, 1.5],\n",
    "])\n",
    "X = np.hstack((np.ones((5, 1)), X))\n",
    "y = np.array([[1], [0], [1], [0], [1]])\n",
    "\n",
    "J, grad = lrCostFunction(X, y, theta)\n",
    "\n",
    "np.testing.assert_allclose(J, np.array([0.73482]), rtol=1e-05)\n",
    "np.testing.assert_allclose(\n",
    "    grad, np.array([[0.14656137],[0.05144159],[0.12472227],[0.19800296]]), rtol=1e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edd0bc",
   "metadata": {},
   "source": [
    "## ここまででex2の演習に一旦移動"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bde79c",
   "metadata": {},
   "source": [
    "## 特徴量正規化\n",
    "\n",
    "それぞれの入力値をほぼ同じ範囲にすることで、勾配降下法を高速化することができます。これは、θが小さい範囲では素早く、大きい範囲ではゆっくりと下降するため、変数が非常に不均一な場合、最適な値まで非効率的に振動してしまうためです。\n",
    "\n",
    "これを防ぐためには、入力変数の範囲を変更して、すべての変数がほぼ同じになるようにすることです。理想的には以下となる\n",
    "\n",
    "$-1 \\leq x_{(i)} \\leq 1 \\ \\ or \\ \\ -0.5 \\leq x_{(i)} \\leq 0.5 $\n",
    "\n",
    "これらは厳密な要件ではなく、あくまでもスピードアップを図るためのものです。目標は、すべての入力変数を、多少の差はあれど、大体これらの範囲に収めることです。\n",
    "\n",
    "そのための手法として、特徴量のスケーリングと平均値の正規化があります。特徴量スケーリングでは、入力値を入力変数の範囲（すなわち、最大値から最小値を引いたもの）で割ることで、新しい範囲が1になります。平均正規化では、入力変数の値からその入力変数の平均値を引くことで、入力変数の新しい平均値が0になります。この2つの手法を実行するには、次の式のように入力値を調整します。\n",
    "\n",
    "$ x_i := \\frac{x_i - \\mu_i}{s_i}$\n",
    "\n",
    "ここで、$\\mu_i$ は特徴量（i）のすべての値の平均値、$s_i$ は値の範囲（最大-最小）、または $s_i$は標準偏差です。\n",
    "\n",
    "範囲で割るのと、標準偏差で割るのとでは、結果が異なることに注意してください。以下は範囲を使った例ですが演習では標準偏差を使用しています。\n",
    "\n",
    "例：$x_i$は住宅価格で、範囲は100から2000、平均値は1000です。そして，$x_i := \\frac{price-1000}{1900}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad531c7",
   "metadata": {},
   "source": [
    "#### ※ YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457bef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    return \n",
    "\n",
    "np.testing.assert_equal(featureNormalize(np.array([[1], [2], [3]])), np.array([[-1.], [0.], [1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c2e3c7",
   "metadata": {},
   "source": [
    "#### ※ END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c9b18",
   "metadata": {},
   "source": [
    "## 更なる最適化、結局出来合いのソルバーを使おうという話\n",
    "\n",
    "「共役勾配」、「BFGS」、「L-BFGS」は、勾配降下法の代わりに使用できる、より洗練された高速なθの最適化方法です。勾配効果法は更新レートやイテレーションといった調整項があり扱いづらい。先ほどの演習で突如現れたニュートン法はそのような調整項がないことがわかりやすい利点だ。さらに目的関数の二回微分計算しながら最適解を探すため勾配降下法に比べてある条件下においては収束しやすい。しかしながら諸々の数学的な理由（二回微分が正定値性を満たさない場合）を考慮するとこれも不十分であり・・・。といった具合に数学シロウトの我々を深淵が待ち受けている。これ以上は学習コストとその見返りのバランスが悪いとの判断から問題に応じてscipyの最適化ソルバーを使っていく方針に切り替えた。（2021/10/13現在）\n",
    "\n",
    "今回使用するのは[scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)、デフォルトではBFGSを使うことになるだろう。BFGSは[準ニュートン法](https://ja.wikipedia.org/wiki/%E6%BA%96%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%88%E3%83%B3%E6%B3%95)とも呼ばれているようで、ニュートン法で都度計算される[ヘッセ行列](https://ja.wikipedia.org/wiki/%E3%83%98%E3%83%83%E3%82%BB%E8%A1%8C%E5%88%97)（要は二階微分）の近似行列$B_k$を用いることでその正定値性（おそらく下に凸であること）を保証しながらパラメーターの更新をすることが可能だ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb27e1",
   "metadata": {},
   "source": [
    "## 多クラス分類\n",
    "\n",
    "ここでは、データを2つ以上のカテゴリーに分類する方法を紹介します。y = {0,1}の代わりに、y = {0,1...n}のように定義を拡張します。\n",
    "\n",
    "この場合、問題をn+1（インデックスが0から始まるので+1）個のバイナリ分類問題に分割し、それぞれにおいて、「y」がクラスの1つのメンバーである確率を予測します。\n",
    "\n",
    "$y∈{0,1...n}$  \n",
    "$h(0)θ(x)=P(y=0|x;θ)$  \n",
    "$h(1)θ(x)=P(y=1|x;θ)$  \n",
    "$⋯$  \n",
    "$h(n)θ(x)=P(y=n|x;θ)$ \n",
    "$prediction=maxi(h(i)θ(x))$  \n",
    "\n",
    "基本的には、1つのクラスを選択し、他のすべてのクラスを1つの第2のクラスにまとめています。これを繰り返し行い、それぞれのケースにバイナリ・ロジスティック回帰を適用し、最も高い値を返した仮説を予測値として使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31344a",
   "metadata": {},
   "source": [
    "## 正則化\n",
    "\n",
    "オーバーフィッティングの問題\n",
    "\n",
    "正則化は、オーバーフィッティングの問題に対処するために設計されている。\n",
    "\n",
    "ハイバイアスやアンダーフィッティングとは、仮説関数hの形がデータの傾向にうまく対応していない場合をいう。これは通常、関数が単純すぎたり、使用する特徴が少なすぎたりすることが原因です。例えば、$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2$とすると 、線形モデルがトレーニングデータによく適合し、一般化できるという初期の仮定をしていますが、実際にはそうではないかもしれません。\n",
    "\n",
    "一方、オーバーフィッティングや高分散は、仮説関数が利用可能なデータにフィットしても、新しいデータを予測するためにうまく一般化できないことが原因で起こります。通常は、データとは関係のない不必要な曲線や角度をたくさん作るような複雑な関数が原因となります。\n",
    "\n",
    "この用語は、線形回帰とロジスティック回帰の両方に適用されます。オーバーフィッティングの問題に対処するには、主に2つの選択肢があります。\n",
    "\n",
    "1) 特徴量の数を減らす。\n",
    "\n",
    "       a) 残すべき特徴を手動で選択する。\n",
    "\n",
    "       b) モデル選択アルゴリズムを使用する（本コースで後述）。\n",
    "\n",
    "2) 正則化\n",
    "\n",
    "すべての特徴量を残すが，パラメータ $\\theta_j$ .正則化は、わずかに有用な特徴がたくさんある場合に有効です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4bc4b",
   "metadata": {},
   "source": [
    "## コスト関数\n",
    "仮説関数のオーバーフィッティングが発生した場合、関数内のいくつかの項のコストを増やすことで、その項が持つ重みを減らすことができます。\n",
    "\n",
    "例えば，次のような関数をより二次的にしたいとします．\n",
    "\n",
    "$ \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3 + \\theta_4 x^4 $\n",
    "\n",
    "ここで$\\theta_3x^3$と$\\theta_4x^4$の影響を排除したいと思います。これらの特徴量を取り除いたり仮設の形を変えたりすることなく、代わりに以下のような形でコスト関数を修正することができます。\n",
    "\n",
    "$ min_{\\theta} \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + 1000 \\cdot \\theta_3^2 + 1000 \\cdot \\theta_4^2$\n",
    "\n",
    "$\\theta_3$と$\\theta_4$のコストを膨らませるために余分に二つの項を追加しました。ここでコスト関数をゼロに近づけるためには$\\theta_3$と$\\theta_4$をゼロに近い値にする必要があります。これは$\\theta_3x^3$と$\\theta_4x^4$を大きく減らすことを意味します。\n",
    "\n",
    "また、すべてのシータ・パラメータを単一の和で正則化することもできます。\n",
    "\n",
    "$ min_{\\theta} \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_{j}^{2}$\n",
    "\n",
    "λ(ラムダ)は正則化パラメータです。シータパラメータのコストをどれだけ膨らませるかを決定します。正則化の効果をこのインタラクティブなプロットで視覚化することができます。\n",
    "\n",
    "https://www.desmos.com/calculator/1hexc8ntqp\n",
    "\n",
    "上記のコスト関数に余分な和を加えることで、仮説関数の出力を平滑化し、オーバーフィッティングを減らすことができます。lambdaを大きくしすぎると，関数を平滑化しすぎてアンダーフィッティングを引き起こす可能性がある．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643281e",
   "metadata": {},
   "source": [
    "## 正則化線形回帰\n",
    "\n",
    "## 正則化ロジスティック回帰\n",
    "\n",
    "ロジスティック回帰の正則化は、線形回帰の正則化と同様の方法で行うことができます。まずは、コスト関数から始めましょう。\n",
    "\n",
    "コスト関数\n",
    "ロジスティック回帰のコスト関数が次のようだったことを思い出してください。\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}[y^{(i)}log(h_\\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$$\n",
    "\n",
    "この等式を一つの項を加えて正則化することができる。\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}[y^{(i)}log(h_\\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]　+ \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2$$\n",
    "\n",
    "上式の第二項$ \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2$はバイアス項である$\\theta_0$を除外することを示している。例えば$\\theta$ベクトルは0からnでインデックス化されている（$\\theta_0$から$\\theta_n$までn+1の値を持つ）、そしてその合計は1からnまでの計算をすることで$\\theta_0$を明示的にスキップしている。\n",
    "\n",
    "#### 勾配降下法\n",
    "\n",
    "線形回帰と同様に$\\theta_0$と残りのパラメータを**別々に**アップデートしよう。なぜなら$\\theta_0$は先述の理由から正則化されないからだ。\n",
    "\n",
    "Repeat {  \n",
    "$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_0^{(i)}$\n",
    "$\\theta_j := \\theta_0 - \\alpha \\left[ \\left( \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j \\right] \\ \\ \\ j \\in \\{1, 2, \\dots n \\}$  \n",
    "}\n",
    "\n",
    "これは、線形回帰で紹介した勾配降下関数と同じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de15ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrCostFunctionReg(X, y, theta, lam):\n",
    "    return \n",
    "\n",
    "   np.testing.assert_allclose(J, np.array([2.534819]), rtol=1e-06)\n",
    "    np.testing.assert_allclose(grad, np.array([[0.146561], [-0.54856], [0.72472], [1.39800]]), rtol=1e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e684b8",
   "metadata": {},
   "source": [
    "## 初期の単位特徴ベクトル\n",
    "\n",
    "### 定数特徴量\n",
    "機械のトレーニングを開始する前に、特徴量のプールに一定の特徴量を追加することが重要であることがわかりました。通常、この特徴量は、すべての学習例に対する1の集合である。\n",
    "具体的には、特徴量行列をXとすると、X_0は「1」のベクトルとなります。\n",
    "\n",
    "以下に、この一定の特徴の理由を説明します。前半は電気工学の概念からの類推で、後半は機械学習の簡単な例を用いて1のベクトルを理解することにします。\n",
    "\n",
    "### 電気工学\n",
    "電気工学、特に信号処理から、これはDCとACとして説明できます。\n",
    "\n",
    "定数項のない初期特徴ベクトルXは、モデルのダイナミクスを捉えています。つまり、これらの特徴は、特に出力yの変化を記録します。言い換えれば、i=0ではないある特徴X_iを変更すると、出力yに変化が生じます。\n",
    "\n",
    "\n",
    "一定の特徴は、直流成分を表しています。制御工学では、これは定常状態とも言えます。\n",
    "興味深いことに、直流項を除去するには、信号を微分するか、離散信号の連続した点の差を取るだけで簡単にできます（この時点では、アナログは時間ベースの信号を意味していることに注意すべきです。\n",
    "\n",
    "この例えは時間ベースのシグナルを意味しているので、時間ベースの機械学習アプリケーション（例：証券取引所の動向の予測）にも意味があるでしょう。)\n",
    "\n",
    "もう1つ面白いことがあります。それは、AC+DCの信号と、ACのみの信号を再生した場合、両方のAC成分が同じであれば、両者は全く同じに聞こえるということです。これは、私たちは信号の変化だけを聞いているからで、Δ(AC+DC)=Δ(AC)なのです。\n",
    "\n",
    "### 住宅価格の例\n",
    "ある特徴に基づいて住宅の価格を予測する機械を設計するとします。この場合、特徴ベクトルは何に役立つのでしょうか？\n",
    "\n",
    "ここでは、期待価格に正比例する特徴を持つ単純なモデルを仮定してみましょう。つまり、特徴Xiが増加すれば、期待価格yも増加するということです。例えば、家の大きさ(m2)と部屋数の2つの特徴があるとします。\n",
    "\n",
    "機械を訓練する際には、まず1つのベクトルX_0を入力します。学習後に、最初の特徴である「1」の重みが、ある値θ0であることがわかるかもしれません。その結果、仮説関数hθ(X)を適用すると、初期特徴の場合は定数（シグモイドなどの他の関数を適用しない場合は、おそらくθ0）を乗じることになります。この定数（仮にθ_0とすると とします）がDC項です。これは変化しない定数です。\n",
    "\n",
    "しかし、この例ではどういう意味があるのでしょうか？さて、あなたが住宅価格のワーキングモデルを持っていることを誰かが知っているとしましょう。この例では、家を売ったらいくらになるかと聞かれたら、学習機械を使う前に、少なくともθ0ドル（またはランド）が必要だと言えることがわかりました。上の例えのように、あなたの定数θ0は、すべての入力がゼロである定常状態のようなものです。具体的には、部屋のない、スペースのない家の値段です。\n",
    "\n",
    "しかし、この説明にはいくつかの穴があります。例えば、年齢などの価格を下げる特徴がある場合、DC項は価格の絶対的な最小値ではないかもしれません。なぜなら、年齢によって価格がさらに下がる可能性があるからです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c3ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
